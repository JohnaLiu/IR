Title       : ITR: Contributions of Eye Movements and Shared Attention to Collaborative Tasks
Type        : Award
NSF Org     : IIS 
Latest
Amendment
Date        : July 1,  2002       
File        : a0082602

Award Number: 0082602
Award Instr.: Continuing grant                             
Prgm Manager: C. Suzanne Iacono                       
	      IIS  DIV OF INFORMATION & INTELLIGENT SYSTEMS
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : September 1,  2000  
Expires     : August 31,  2003     (Estimated)
Expected
Total Amt.  : $498989             (Estimated)
Investigator: Susan E. Brennan susan.brennan@sunysb.edu  (Principal Investigator current)
              Richard J. Gerrig  (Co-Principal Investigator current)
              Gregory J. Zelinsky  (Co-Principal Investigator current)
Sponsor     : SUNY Stony Brook
	      
	      Stony Brook, NY  117943362    631/632-9949

NSF Program : 1640      INFORMATION TECHNOLOGY RESEARC
Fld Applictn: 0104000   Information Systems                     
              0116000   Human Subjects                          
Program Ref : 1657,1660,9218,HPCC,
Abstract    :
              During remote collaboration, partners have access to much less information than
              during face-to-face collaboration. This project uses psychological experiments
              to examine how knowing where a partner is looking affects performance and
              strategies on collaborative tasks. Phase 1 asks: When partners are physically
              co-present, how aware are they of where the other is attending, and how do they
              achieve shared attention?  Phase II applies these basic results to remote
              collaborations; partners wear eye-trackers that transmit gaze information to
              each other's computer displays.  A space of tasks and representations is
              explored: Tasks are varied in systematic ways (e.g., some lend themselves to
              parallel activity, while others require consensus for each step), and different
              representations of the same gaze information are compared.  The goal is to
              understand which representations work best for which tasks.  With technological
              advances making eyetracking easier, less cumbersome, and more affordable, a
              gaze-based computer interface may someday join the ranks of ubiquitous input
              devices like the mouse.  If this technology is to be integrated into the "every
              citizen interface," it is necessary to understand how people use the
              information in gaze to achieve a joint focus of attention. This could provide
              the foundations for new technology for computer-mediated collaboration.
