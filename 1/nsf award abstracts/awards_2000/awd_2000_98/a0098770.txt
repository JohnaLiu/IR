Title       : Collaborative Research: Globally Optimal Neural Computing: Algorithms and
               Applications
Type        : Award
NSF Org     : ECS 
Latest
Amendment
Date        : August 2,  2001     
File        : a0098770

Award Number: 0098770
Award Instr.: Standard Grant                               
Prgm Manager: Paul Werbos                             
	      ECS  DIV OF ELECTRICAL AND COMMUNICATIONS SYS
	      ENG  DIRECTORATE FOR ENGINEERING             
Start Date  : August 1,  2001     
Expires     : July 31,  2003       (Estimated)
Expected
Total Amt.  : $158457             (Estimated)
Investigator: Nikolaos V. Sahinidis nikos@uiuc.edu  (Principal Investigator current)
Sponsor     : U of Ill Urbana-Champaign
	      801 South Wright Street
	      Champaign, IL  61820    217/333-2186

NSF Program : 1518      CONTROL, NETWORKS, & COMP INTE
Fld Applictn: 0510403   Engineering & Computer Science          
Program Ref : 0000,OTHR,
Abstract    :
              0098770
Sahinidis

This grant supports a collaboration between a member of
              the global optimization community (Nick Sahinidis) and an expert in neural
              computation and optimization (Theodore Trafalis) to develop novel neural
              network training algorithms and demonstrate their benefits in solving
              large-scale learning) problems.

The application of neural networks to all
              aspects of technology has escalated recently as engineers and scientists have
              widely embraced neural computing in their quest for deeper understanding of
              complex phenomena and systems.

Finding the best possible neural network for
              a particular application requires choosing the network parameters in a way that
              minimizes learning errors. Even for simple learning problems, the error
              function possesses a large number of local minima (isolated valleys). Despite
              the enormous amount of attention devoted to neural networks, there is currently
              no efficient method that can identify with certainty time global minimum of the
              error function. Current approaches, such as back-propagation and stochastic
              search methods, may get trapped at local minima corresponding to large learning
              errors and suboptimal neural networks. This may lead to incorrect inferences
              and devastate decision makers.

Globally optimal neural computing holds the
              promise of an enabling technology that could significantly improve learning in
              many diverse application domains.  The results of the proposed research will be
              implemented in the their widely distributed global optimization software
              package and will be made available to the research community.

