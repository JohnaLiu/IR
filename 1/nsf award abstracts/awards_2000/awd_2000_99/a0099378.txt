Title       : Collaborative Research: Globally Optimal Neural Computing: Algorithms and
               Applications
Type        : Award
NSF Org     : ECS 
Latest
Amendment
Date        : August 2,  2001     
File        : a0099378

Award Number: 0099378
Award Instr.: Standard Grant                               
Prgm Manager: Paul Werbos                             
	      ECS  DIV OF ELECTRICAL AND COMMUNICATIONS SYS
	      ENG  DIRECTORATE FOR ENGINEERING             
Start Date  : August 1,  2001     
Expires     : July 31,  2003       (Estimated)
Expected
Total Amt.  : $150553             (Estimated)
Investigator: Theodore B. Trafalis ttrafalis@ou.edu  (Principal Investigator current)
Sponsor     : U of Oklahoma
	      1000 Asp Avenue, Room 314
	      Norman, OK  73019    405/325-4757

NSF Program : 1518      CONTROL, NETWORKS, & COMP INTE
Fld Applictn: 0510403   Engineering & Computer Science          
Program Ref : 0000,OTHR,
Abstract    :
              0099378
Trafalis

This grant supports a collaboration between a member of the
              global optimization community (Nick Sahinidis) and an expert in neural
              computation and optimization (Theodore Trafalis) to develop novel neural
              network training algorithms and demonstrate their benefits in solving
              large-scale learning) problems.

The application of neural networks to all
              aspects of technology has escalated recently as engineers and scientists have
              widely embraced neural computing in their quest for deeper understanding of
              complex phenomena and systems.

Finding the best possible neural network for
              a particular application requires choosing the network parameters in a way that
              minimizes learning errors. Even for simple learning problems, the error
              function possesses a large number of local minima (isolated valleys). Despite
              the enormous amount of attention devoted to neural networks, there is currently
              no efficient method that can identify with certainty time global minimum of the
              error function. Current approaches, such as back-propagation and stochastic
              search methods, may get trapped at local minima corresponding to large learning
              errors and suboptimal neural networks. This may lead to incorrect inferences
              and devastate decision makers.

Globally optimal neural computing holds the
              promise of an enabling technology that could significantly improve learning in
              many diverse application domains.  The results of the proposed research	will be
              implemented in the their widely distributed global optimization software
              package and will be made available to the research community.

