Title       : REU: CONACyT: Speech Driven Facial Animation
Type        : Award
NSF Org     : IIS 
Latest
Amendment
Date        : July 13,  2002      
File        : a9906340

Award Number: 9906340
Award Instr.: Standard Grant                               
Prgm Manager: Ephraim P. Glinert                      
	      IIS  DIV OF INFORMATION & INTELLIGENT SYSTEMS
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : September 1,  1999  
Expires     : August 31,  2003     (Estimated)
Expected
Total Amt.  : $209287             (Estimated)
Investigator: Oscar N. Garcia ogarcia@cs.wright.edu  (Principal Investigator current)
              A. Ardeshir Goshtasby  (Co-Principal Investigator current)
              Ricardo Gutierrez-Osuna  (Co-Principal Investigator current)
Sponsor     : Wright State University
	      3640 Colonel Glenn Highway
	      Dayton, OH  454350001    937/775-2425

NSF Program : 6846      UNIVERSAL ACCESS
Fld Applictn: 0104000   Information Systems                     
Program Ref : 1069,9216,HPCC,
Abstract    :
              This project will produce a faithful 3D animation of the face of a speaker,
              accurately articulating visible speech, and preserving the natural synchronism
              between the utterances of the speaker and the corresponding facial movements,
              including as much of the gestural prosody as possible.  This is accomplished
              through a series of transformations from the speech input to the display.  From
              the phonetic perspective the PIs will study the relation between phonemes and
              visemes at a level of granularity below the phoneme.   From the computer
              graphics perspective, they will model the facial features so that realistic
              synchronism with the utterances takes place using the computer vision technique
              of active contours and the animation technique of multilayer implicit models. 
              Finally, from a prosodic point of view they will apply the concepts of gestural
              scores which should account, on a more principled basis, for the prosodic
              phenomena and contextual influences in speech.  The gestural model has evolved
              into a novel task-dynamic approach to invariant articulatory gestures based on
              an orchestration (by means of a rather parsimonious "score") of a second order
              spring-mass system reaching for target specific goals.  An acoustic features
              table includes the phasing and duration which characterize gestural scores. 
              This work has important implications not only for animation (films and
              television, interfaces for the hearing impaired, low bandwidth telephony,
              repeatable prosody), but also for basic linguistic modeling of prosody across
              the two languages (English and Spanish) being considered.
