Title       : An Integrated Linguistic-Computational Approach to Automatic Recognition of
               American Sign Language
Type        : Award
NSF Org     : BCS 
Latest
Amendment
Date        : December 14,  2001  
File        : a9905848

Award Number: 9905848
Award Instr.: Continuing grant                             
Prgm Manager: Cecile McKee                            
	      BCS  DIVISION OF BEHAVIORAL AND COGNITIVE SCI
	      SBE  DIRECT FOR SOCIAL, BEHAV & ECONOMIC SCIE
Start Date  : November 1,  1999   
Expires     : October 31,  2003    (Estimated)
Expected
Total Amt.  : $503999             (Estimated)
Investigator: Ronnie B. Wilbur   (Principal Investigator current)
              Avinash C. Kak  (Co-Principal Investigator current)
Sponsor     : Purdue Research Foundation
	      
	      West Lafayette, IN  47907    317/494-6200

NSF Program : 1311      LINGUISTICS
Fld Applictn: 0116000   Human Subjects                          
Program Ref : 0000,OTHR,
Abstract    :
              This project offers a novel approach to the problem of automatic recognition,
              and eventually automatic translation, of American Sign Language (ASL). The
              approach takes advantage of the fact that signs are composed of components
              (handshape, location, orientation, movement), in much the same way that words
              are composed of consonants and vowels. Instead of attempting sign recognition
              as a process of matching the input against a stored set of lexical signs, sign
              recognition will be treated as the end result of several separate but
              interrelated component recognition procedures, of which the current project
              focuses on handshape recognition.

This design eliminates the enormous
              storage and processing-time difficulties associated with storing and retrieving
              3-D moving images of signs in a look-up lexicon. Hand motion will be
              represented by the parameters of Hidden Markov Models; additionally, the
              approach aims to identify an input handshape from among the set of possible ASL
              handshapes. When coupled with similar procedures that identify locations,
              orientation, and movement, the composite identified set of components should
              yield a single lexical sign in a dictionary look-up.

The project uniquely
              integrates several areas of basic research: linguistic research on the
              structure of signs in ASL, psycholinguistic research on human perception of
              ASL, and advanced techniques from statistical pattern recognition and computer
              vision to analyze input from stereo images of signers. The procedure is
              designed so that linguistic information about ASL handshapes can be used to
              assist the computer in deciding which handshape it "sees". The long-term goal
              is to integrate the recognition procedure as the user interface of an
              ASL-English machine translation device. Such a device would support
              interactions between signers and speakers in practical settings, including the
              workplace and classrooms.
