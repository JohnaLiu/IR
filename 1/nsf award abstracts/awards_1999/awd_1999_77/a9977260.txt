Title       : Computer-Based Speech Training for the Hearing Impaired
Type        : Award
NSF Org     : BES 
Latest
Amendment
Date        : June 25,  2001      
File        : a9977260

Award Number: 9977260
Award Instr.: Continuing grant                             
Prgm Manager: Leon Esterowitz                         
	      BES  DIV OF BIOENGINEERING & ENVIRON SYSTEMS 
	      ENG  DIRECTORATE FOR ENGINEERING             
Start Date  : September 15,  1999 
Expires     : August 31,  2003     (Estimated)
Expected
Total Amt.  : $261035             (Estimated)
Investigator: Stephen A. Zahorian   (Principal Investigator current)
Sponsor     : Old Dominion Research Fdn
	      2033 Hughes Hall
	      Norfolk, VA  23529    757/683-4293

NSF Program : 5342      RESEARCH TO AID THE DISABLED
Fld Applictn: 0116000   Human Subjects                          
              0203000   Health                                  
Program Ref : 0000,OTHR,
Abstract    :
              9977260
Zahorian
The objectives of the proposed research are to investigate
              and optimize methods for extracting speaker-independent acoustically-based
              speech parameters that signal the phonetic content of speech and to convert
              these parameters to a visual display for use as an articulation training aid
              for the hearing impaired.  The proposed project is a continuation and extension
              of research previously funded by the NSF.  

A major component of the effort
              will be to collect and annotate a large database of speech samples from both
              normally-hearing and hearing-impaired listeners, for use with the proposed work
              and also to facilitate other research efforts in this field.  Extensive
              experiments will be conducted to optimize the conversion of speech parameters
              to display parameters so that phonemes and distinctive features of phonemes,
              produced either in isolation, syllabic contexts, or as part of continuous
              speech, can readily be discriminated and identified based on their display
              characteristics.  Automatic speech recognition algorithms will be developed and
              refined to create a visual display representation.   Acoustic features will be
              extracted from the peak envelope spectrum, fundamental frequency, and
              short-time energy.   Several recognition strategies, all using neural networks
              as a key building block, will be investigated.    Visual displays will be
              developed and tested for several speech training exercises at various levels of
              complexity-ranging from distinctive features for phones produced in isolation
              to phonetic displays for continuous speech.   One display mode will be
              developed which provides a continuous relationship between articulatory
              features for vowel and consonant production and displayed patterns, and will
              show both degree and directions of any needed corrections in articulation. 
              Additionally, discrete displays, such as bargraphs or games, will indicate the
              closeness of a production to a "normal" production.  

The emphasis of the
              research is to improve automatic phonetic recognition to the level that the
              feedback given by the algorithm is as accurate and consistent as that which
              would be possible by an expert experienced listener.  Controlled evaluation
              experiments will be conducted to determine the extent to which "new" speech
              articulation patterns can be learned on the basis of the feedback given from
              the various displays.  Speech articulation training exercises will also be
              conducted with hearing-impaired children to assess the usefulness of the visual
              display in improving speech production and to help determine methods to improve
              it.  









