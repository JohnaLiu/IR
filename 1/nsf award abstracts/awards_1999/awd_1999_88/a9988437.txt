Title       : GOALI: Haptic Cobots
Type        : Award
NSF Org     : IIS 
Latest
Amendment
Date        : July 2,  2002       
File        : a9988437

Award Number: 9988437
Award Instr.: Continuing grant                             
Prgm Manager: Junku Yuh                               
	      IIS  DIV OF INFORMATION & INTELLIGENT SYSTEMS
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : June 15,  2000      
Expires     : May 31,  2003        (Estimated)
Expected
Total Amt.  : $447650             (Estimated)
Investigator: Michael A. Peshkin peshkin@nwu.edu  (Principal Investigator current)
              Paul J. Stewart  (Co-Principal Investigator current)
              Pietro Buttolo  (Co-Principal Investigator current)
              J. Edward Colgate  (Co-Principal Investigator current)
Sponsor     : Northwestern University
	      633 Clark Street
	      Evanston, IL  602081110    847/491-3003

NSF Program : 6840      ROBOTICS AND HUMAN AUGMENTATIO
Fld Applictn: 0104000   Information Systems                     
Program Ref : 1504,9216,HPCC,
Abstract    :
              We propose research underlying the use of cobots for  haptic display of solid
              models. The project brings together CAD graphics/haptics researchers at Ford
              Motor Company with haptics/cobot researchers at Northwestern University. A
              recently completed 3R haptic cobot will be the experimental testbed for the
              project.

Large-scale haptic display opens up new opportunities, because
              people interact with large objects (arm size or larger) in a very different way
              from that in which they interact with small objects (hand size or smaller).  An
              example is the current use of full-size "clay bucks" in automobile design. A
              sense of the feel and sweep of an automobile body panels cannot be obtained by
              touching a scale model of it with a finger, as current haptic displays permit.
              However a full sized virtual model experienced through the proprioception of
              whole arm motion, in conjunction with the excellent CAD graphics now available,
              could bring virtual prototyping and surface editing to a new level of
              utility.

Cobot control for haptics differs markedly from robot control for
              haptics, because cobots use servo-steered rolling mechanisms, rather than
              servomotor actuators, to create virtual surfaces. The project addresses (1)
              development of a control methodology for  the new power-injection architecture
              of the 3R cobot.  (2) finding cobot-appropriate algorithms for haptic surface
              rendering directly from NURBS descriptions of surfaces.  (3) deriving control
              laws for dynamic behaviors beyond hard surfaces, including compliant and
              viscous effects, inertia masking, and artificial potentials (4) finding
              algorithms for solid-model collision detection which allow the collision to be
              predicted and rendered by the cobot without exceeding its dynamic limits.



