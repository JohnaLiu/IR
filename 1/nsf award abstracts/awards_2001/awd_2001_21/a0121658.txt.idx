Title       : ITR/AP Methodologies and Tools for Designing and Implementing Large Scale
               Real-Time Systems
Type        : Award
NSF Org     : ACI 
Latest
Amendment
Date        : September 24,  2001 
File        : a0121658

Award Number: 0121658
Award Instr.: Continuing grant                             
Prgm Manager: Charles H. Koelbel                      
	      ACI  DIV OF ADVANCED COMPUT INFRA & RESEARCH 
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : October 1,  2001    
Expires     : September 30,  2006  (Estimated)
Expected
Total Amt.  : $4978000            (Estimated)
Investigator: Paul D. Sheldon Paul.Sheldon@Vanderbilt.edu  (Principal Investigator current)
              Jae Oh  (Co-Principal Investigator current)
              Ruth Pordes  (Co-Principal Investigator current)
              Michael J. Haney  (Co-Principal Investigator current)
              Theodore A. Bapty  (Co-Principal Investigator current)
Sponsor     : Vanderbilt University
	      512 Kirkland Hall
	      Nashville, TN  37240    615/322-2631

NSF Program : 1687      ITR MEDIUM (GROUP) GRANTS
Fld Applictn: 0000099   Other Applications NEC                  
Abstract    :
              This proposal requests support for research to develop methodologies and tools
              for designing and implementing very large-scale real-time embedded computer
              systems that (a) achieve ultra high computational performance through use of
              parallel hardware architectures, (b) achieve and maintain functional integrity
              via distributed, hierarchical monitoring and control, (c) are required to be
              highly available, and (d) are dynamically reconfigurable, maintainable, and
              evolvable.  The specific application that will drive this research and provide
              a test platform for it is the trigger and data acquisition system for BTeV, an
              accelerator-based High Energy Physics (HEP) experiment to study
              matter-antimatter asymmetries (also known as Charge-Parity violation) in the
              decays of particles containing the b-quark.  BTeV has been approved by Fermilab
              management and is expected to be constructed over the next 5-6 years to run in
              conjunction with the Fermilab Tevatron Collider.  The data-taking phase of the
              experiment is expected to be at least five years.  It requires a massively
              parallel, heterogeneous cluster of computing elements to reconstruct 15 million
              particle interactions (events) per second and to use the reconstruction data to
              decide which events to retain for further data analysis.    Creating usable
              software for this type of real-time embedded system will require research into
              solutions of general problems in the fields of computer science and
              engineering.  The proponents plan to approach these problems in a way that is
              general, and to produce methodologies and tools that can be applied to many
              scientific and commercial problems.  During this project, the research results
              will be carried into the high-school system through projects, which enhance
              existing infrastructure for attracting students into science and engineering
              disciplines.   The classes of systems targeted by this research include those
              imbedded in environments, like BTeV, that produce very large data streams which
              must be processed in real-time using data-dependent computation strategies. 
              These systems require ultra high performance (~1012 operations per second),
              necessitating parallel hardware architectures, which in the case of BTeV is
              composed of a mix of thousands of commodity processors, special purpose
              processors such as Digital Signal Processors (DSPs), and specialized hardware
              such as Field Programmable Gate Arrays (FPGAs), all connected by very
              high-speed networks.  The systems must be dynamically reconfigurable to allow
              maximum performance from the available and potentially changing resources.  The
              systems must be highly available, since the environments produce the data
              streams continuously over a long period of time, and interesting phenomena
              important to the analysis are rare and could occur at any time.  To achieve the
              high availability, the systems must be fault tolerant, self-aware, and fault
              adaptive, since any malfunction of processing elements, the interconnection
              switches, or the front-end sensors (which provide the input stream) can result
              in an unrecoverable loss of data.  Faults must be corrected in the shortest
              possible time, and corrected semi-autonomously (i.e., with as little human
              intervention as possible).  Hence, distributed and hierarchical monitoring and
              control are vital.      
