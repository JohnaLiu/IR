Title       : ITR: Bayesian Learning at the Syntax-Semantics Interface
Type        : Award
NSF Org     : IIS 
Latest
Amendment
Date        : August 30,  2002    
File        : a0218852

Award Number: 0218852
Award Instr.: Continuing grant                             
Prgm Manager: Karen Kukich                            
	      IIS  DIV OF INFORMATION & INTELLIGENT SYSTEMS
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : September 1,  2002  
Expires     : July 31,  2004       (Estimated)
Expected
Total Amt.  : $400000             (Estimated)
Investigator: Robert C. Berwick berwick@ai.mit.edu  (Principal Investigator current)
              Jesse Snedeker  (Co-Principal Investigator current)
Sponsor     : MIT
	      77 Massachusetts Avenue
	      Cambridge, MA  021394307    617/253-1000

NSF Program : 1686      ITR SMALL GRANTS
Fld Applictn: 0104000   Information Systems                     
              0116000   Human Subjects                          
Program Ref : 1654,9216,HPCC,
Abstract    :
              Bayesian Learning at the Syntax-Semantics Interface
Abstract

Children easily
              learn features of novel verbs from small numbers of scene-utterance pairs. For
              example, after encountering a few examples of "breaking" an object, they infer
              that break might require an object, e.g., John broke the glass. They also learn
              semantic properties. Children and adults can then generalize to other scene
              instances representing break.   This project hypothesizes that children combine
              syntactic and semantic evidence to learn verb features, using a probabilistic
              method called Bayesian inference. 

The project's first goal is to implement
              a computational model that can induce probability distributions on features
              from a very small number of scene-utterance pairs. This model will make
              explicit all the information sources used. Second, the project will confirm
              which cues are actually used by human learners in certain settings. The
              experimental method matches the computer model's predictions empirically, by
              presenting adult and child learners with training sequences of novel verbs used
              across varying syntactic and semantic feature situations.  This project's
              results will advance adaptable computer systems and information-filtering, 
              both in terms of robustness to noise and an ability to learn from a small
              number of examples. These results will improve the construction of a key
              component of natural language processing engines: the dictionary.

