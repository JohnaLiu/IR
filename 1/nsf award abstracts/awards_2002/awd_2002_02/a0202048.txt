Title       : CISE Research Infrastructure: A Research Infrastructure for Collaborative,
               High-Performance Grid Applications
Type        : Award
NSF Org     : EIA 
Latest
Amendment
Date        : December 17,  2002  
File        : a0202048

Award Number: 0202048
Award Instr.: Continuing grant                             
Prgm Manager: Rita V. Rodriguez                       
	      EIA  DIVISION OF EXPERIMENTAL & INTEG ACTIVIT
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : September 1,  2002  
Expires     : June 30,  2007       (Estimated)
Expected
Total Amt.  : $1311875            (Estimated)
Investigator: David S. Wise dswise@cs.indiana.edu  (Principal Investigator current)
              Beth Plale  (Co-Principal Investigator current)
              Geoffrey Fox  (Co-Principal Investigator current)
              Randall Bramley  (Co-Principal Investigator current)
              Andrew Lumsdaine  (Co-Principal Investigator current)
Sponsor     : Indiana University
	      P O Box 1847
	      Bloomington, IN  474021847    812/855-0516

NSF Program : 2885      CISE RESEARCH INFRASTRUCTURE
Fld Applictn: 0000099   Other Applications NEC                  
Program Ref : 9218,HPCC,
Abstract    :
              0202048
Wise, David S.
Indiana University - Bloomington
	
RI: A Research
              Infrastructure for Collaborative, High-Performance Grid Applications

This
              project, developing an experimental infrastructure for distributed high
              performance computing, supports ten research projects extending the
              location-transparency that the Grid provides for computation resources to the
              full spectrum of activities which end-users require. Services being explored
              include software development, parallel code middleware, distributed software
              components for scientific computing, security for parallel remote method
              invocation, 
managing large-scale data streams, and collaboration
              methodologies. The research builds on and extends the institutions
              collaborations with several national Grid research teams. In contrast to
              existing national and university infrastructure available through production
              machines, this research requires an environment tolerant of experimental
              network protocols, temporary middleware, and other system-level changes. The
              infrastructure will contribute to the following research projects:
a. Opie: 
              basic work on parallel matrix algorithms that achieve high efficiency across
              many architectural platforms
b. LAM: middleware MPI implementations supporting
              hierarchical and fault-tolerant parallel computing
c. dQUOB: application of
              SQL queries to live data streams
d. RMI Security: basic research into security
              mechanisms for remote method invocation, allowing security to be traded off
              with efficiency
e. HPJ: High Performance Java creating a language platform for
              portable high performance coding
f. Grid Broker: reliable, robust
              publish/subscribe service for introducing fault tolerance into the distributed
              Grid environment
g. Community Grids Collaboratory: advanced collaboration
              capabilities with applications to both distance education and distributed
              communities
h. Xports: design of methodologies for remote instrument access
              and data management of the resulting extremely large data sets
i. Software
              Components: distributed software component model designed for applications that
              use parallel computing "nodes" in wide-area Grid environments
j. Science
              Portals: set of tools that allow programmers to build Grid distributed
              applications accessed and controlled from desktop environments and web
              browsers
Major improvements to infrastructure supporting all these projects
              include a 16-node cycle server and a large-scale file server as well as network
              upgrades to and within the building. 

