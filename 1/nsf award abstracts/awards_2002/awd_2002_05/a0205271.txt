Title       : ITR/AITS: Customizable Audio User Interfaces for the Visually Impaired and the
               Sighted
Type        : Award
NSF Org     : IIS 
Latest
Amendment
Date        : August 21,  2002    
File        : a0205271

Award Number: 0205271
Award Instr.: Continuing grant                             
Prgm Manager: Bhavani Thuraisingham                   
	      IIS  DIV OF INFORMATION & INTELLIGENT SYSTEMS
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : September 1,  2002  
Expires     : June 30,  2007       (Estimated)
Expected
Total Amt.  : $1800000            (Estimated)
Investigator: Ramani Duraiswami ramani@umiacs.umd.edu  (Principal Investigator current)
              Larry S. Davis  (Co-Principal Investigator current)
              Robert W. Massof  (Co-Principal Investigator current)
              Shihab A. Shamma  (Co-Principal Investigator current)
              Ben Shneiderman  (Co-Principal Investigator current)
Sponsor     : U of MD College Park
	      3112 Lee Building
	      College Park, MD  207425141    301/405-6269

NSF Program : 1687      ITR MEDIUM (GROUP) GRANTS
Fld Applictn: 0104000   Information Systems                     
Program Ref : 1655,9216,HPCC,
Abstract    :
              Although large parts of our brains are devoted to the processing of sound cues
              and sound plays an important role in the way we interface with the world, this
              rich channel has not been extensively exploited for displaying information. The
              mechanisms by which received sound waves are processed neurally to form objects
              with auditory properties in many perceptual dimensions, including three
              corresponding to the source location (range, azimuth, elevation) and three
              to	qualities ascribed to the source (timbre, pitch and intensity), are
              beginning to be understood. There has been significant progress over the last
              decade in understanding the mechanisms by which acoustical cues arise and how
              the biological system performs transduction and neural processing to extract
              relevant features from sound, and in the way we perceive and organize objects
              in acoustical scenes. Our goal is to exploit this understanding, and uncover
              the scientific principles that govern the computerized rendering of artificial
              sound scenes containing multiple sound objects that are information and feature
              rich. We will test, use and extend this knowledge by creating auditory user
              interfaces for the visually impaired and the sighted. The work aims both at
              developing interfaces and answering fundamental questions such as: Is it
              possible to usefully map "X" to the auditory axes of a virtual auditory space?
              Here "X" could be an image (e.g., a face), a map, tabular data, uncertain data,
              or temporally varying data. Are there neural correlates that can guide natural
              mappings to acoustic cues? What limitations does our perception place on
              rendering hardware?  How important is 
	




