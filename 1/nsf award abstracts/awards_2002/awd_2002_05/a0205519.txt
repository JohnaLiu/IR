Title       : ITR/ANI: Addressing Fundamental Issues for Robust Internet Performance
Type        : Award
NSF Org     : ANI 
Latest
Amendment
Date        : September 16,  2002 
File        : a0205519

Award Number: 0205519
Award Instr.: Continuing grant                             
Prgm Manager: Taieb F. Znati                          
	      ANI  DIV OF ADVANCED NETWOR INFRA & RESEARCH 
	      CSE  DIRECT FOR COMPUTER & INFO SCIE & ENGINR
Start Date  : September 15,  2002 
Expires     : July 31,  2007       (Estimated)
Expected
Total Amt.  : $2999937            (Estimated)
Investigator: Scott Shenker shenker@icsi.berkeley.edu  (Principal Investigator current)
              Sally Floyd  (Co-Principal Investigator current)
              Vern Paxson  (Co-Principal Investigator current)
              Mark Handley  (Co-Principal Investigator current)
Sponsor     : Internationl CompSci Inst
	      1947 Center Street
	      Berkeley, CA  947041105    510/666-2900

NSF Program : 1687      ITR MEDIUM (GROUP) GRANTS
Fld Applictn: 0206000   Telecommunications                      
Program Ref : 1656,9218,HPCC,
Abstract    :
              The Internet is large, decentralized, and heterogeneous in its technology,
              administration and capacity. The core of the Internet's success arises from its
              adherence to a number of architectural principles, central to which is the
              notion that the network should try to achieve a robust, very often works pretty
              well level of performance. One of the main techniques for achieving this across
              a wide range of conditions is making Internet protocols and mechanisms
              adaptive, so that they self-tune to work reasonably well in whatever
              circumstances they find themselves.

This approach has been extremely
              successful. However, some of the mechanisms, designed to be good enough across
              a large range of conditions, must now or will soon operate in regimes beyond
              their effective dynamic range. For example, TCP congestion control mechanisms
              require revisiting for tomorrow's Internet, both the coming high speed paths,
              and the coming low speed paths (e.g., lossy wireless links). Similarly, the
              architecture's ability to gracefully tolerate failures does not extend to new
              forms of failures, such as misconfigured routing information or malfunctioning
              middle-boxes, nor to distributed stresses, such as flash crowds,
              rapidly-spreading worms, or denial-of-service (DoS) floods.

If we view
              robustness as the ability of the network to function well over a wide spectrum
              of conditions particularly given a very large, ever-growing and ever-changing
              network then we argue that the robustness of the future Internet is clearly at
              risk. In this proposal, we emphasize a multifaceted approach to robustness:
1)
                 Robust performance in the presence of extreme environments such as very high
              speed and highly variable delay, which requires rethinking today's congestion
              control mechanisms.
2)    Robust performance in the presence of new forms of
              failure, both at the network layer, in terms of robust routing, and at the
              application layer, in terms of coping with the now widespread deployment of
              middleboxes that have elbowed their way into the architecture. This will
              require investigating broader notions of fault inference.
3)   Robust
              performance in the presence of distributed stresses, both malicious
              (denial-of-service floods; congestion control cheaters) and merely teeming
              (flash crowds). This will require an understanding of the network's topology
              and the makeup of traffic aggregates, coupled with new control mechanisms
              deployed inside the network.

Part of the approach to these is to refine
              existing protocols and mechanisms, and investigate new ones. But the
              researchers also emphasize achieving robust performance by detecting incipient
              failures, on both short time scales (via distributed operational monitoring)
              and long time scales (via diagnostic probing of deployed protocol
              implementations).

While certainly these topics do not address the full range
              of challenges facing the Internet architecture, they do address some of the
              core issues in preserving and enhancing the Internet's robustness. In one
              sense, the proposed research is conservative, in that we frame it in terms of
              working within the current Internet architecture, rather than advocating a
              clean sheet approach. The researchers argue, however, that in some ways this
              conservative approach makes for research that is more fundamental rather than
              less. The clean sheet approach, while more tidy and much more conducive to easy
              exploration of basic principles, misses the crucial reality of how different
              mechanisms wind up interacting once integrated into a truly large-scale
              network.
